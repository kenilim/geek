import os
import csv
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service as EdgeService
from webdriver_manager.microsoft import EdgeChromiumDriverManager
from bs4 import BeautifulSoup
import time

def setup_driver():
    service = EdgeService('/usr/local/bin/msedgedriver')
    options = webdriver.EdgeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    driver = webdriver.Edge(service=service, options=options)
    return driver

def scrape_category(driver, category_url):
    items = []
    page_number = 1

    while True:
        current_url = f"{category_url}&p={page_number}" if page_number > 1 else category_url
        print(f"Opening URL: {current_url}")
        driver.get(current_url)
        time.sleep(5)  # Wait for the page to load

        print(f"Current URL after loading: {driver.current_url}")
        print(f"Page Title: {driver.title}")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        cards = soup.find_all('div', class_='active-card h-100 card')
        
        print(f"Number of cards found: {len(cards)}")
        if not cards:
            print("No cards found on the page. Check the page structure or the URL.")
            break

        for card in cards:
            try:
                title_div = card.find('div', class_='ms-2 my-3 card-title h5')
                if title_div:
                    name = title_div.text.strip()
                else:
                    name = "No Title Found"

                description_p = card.find('p', class_='ellipsis')
                if description_p:
                    description = description_p.text.strip()
                else:
                    description = "No Description Found"

                link_a = card.find('a', class_='link-secondary text-decoration-none stretched-link')
                if link_a:
                    link = link_a['href']
                else:
                    link = "No Link Found"

                items.append({
                    'Name': name,
                    'Description': description,
                    'Link': f"https://www.sgsocialsupport.com{link}"
                })
            except Exception as e:
                print(f"Error processing card: {e}")

        next_button = driver.find_elements(By.XPATH, '//button[@aria-label="Go to next page"]')
        next_button_disabled = next_button[0].get_attribute('disabled') if next_button else 'true'
        print(f"Next button found: {len(next_button) > 0}, Disabled: {next_button_disabled}")
        
        if next_button and next_button_disabled is None:
            page_number += 1
            print(f"Loading next page: {page_number}")
        else:
            print("No more pages to load or next button is disabled.")
            break

    return items

def scrape_details(driver, url):
    print(f"Scraping details from URL: {url}")
    driver.get(url)
    time.sleep(5)  # Wait for the page to load
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    details = {'Source URL': url}

    try:
        content_div = soup.find('div', class_='container')
        if content_div:
            details['Full Content'] = content_div.get_text(separator=' ', strip=True)
        else:
            details['Full Content'] = "No Content Found"
    except Exception as e:
        print(f"Error extracting details: {e}")

    # Extract specific sections
    sections = ['Who can apply', 'What to expect', 'How to apply', 'Contact']
    for section in sections:
        try:
            header_tag = soup.find('h2', string=section)
            if header_tag:
                section_content = []
                for sibling in header_tag.find_next_siblings():
                    if sibling.name == 'h2':
                        break
                    section_content.append(sibling.get_text(separator=' ', strip=True))
                details[section] = ' '.join(section_content)
            else:
                details[section] = "Not Found"
        except Exception as e:
            details[section] = f"Error: {e}"

    return details

def main():
    base_url = "https://www.sgsocialsupport.com/directory?cat="
    categories = ["Children", "Elderly", "Employment", "Family Service", "Financial", 
                  "Food", "Grants", "Helplines", "Items", "Legal", "Mental Health", 
                  "Minorities", "Physical Health", "Shelter", "Social Enterprises", 
                  "Support Group", "Volunteering", "Misc"]

    driver = setup_driver()

    for category in categories:
        category_url = f"{base_url}{category}"
        print(f"Scraping category: {category}")

        items = scrape_category(driver, category_url)

        if items:
            print(f"Scraped {len(items)} items from category {category}. Data saved to {category}_data.csv")
            with open(f"{category}_data.csv", 'w', newline='') as csvfile:
                fieldnames = ['Name', 'Description', 'Link', 'Full Content', 'Who can apply', 'What to expect', 'How to apply', 'Contact', 'Source URL']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for item in items:
                    details = scrape_details(driver, item['Link'])
                    writer.writerow({**item, **details})
        else:
            print(f"No items scraped from category {category}.")

    driver.quit()

if __name__ == "__main__":
    main()
